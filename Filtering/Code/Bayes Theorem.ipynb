{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed the math in this chapter merely by reasoning about the information we have at each moment. In the process we discovered Bayes Theorem. Bayes theorem tells us how to compute the probability of an event given previous information. That is exactly what we have been doing in this chapter. With luck our code should match the Bayes Theorem equation!\n",
    "\n",
    "We implemented the update() function with this probability calculation:\n",
    "\n",
    "$$ \\mathtt{posterior} = \\frac{\\mathtt{likelihood}\\times \\mathtt{prior}}{\\mathtt{normalization}}$$\n",
    "To review, the prior is the probability of something happening before we include the probability of the measurement (the likelihood) and the posterior is the probability we compute after incorporating the information from the measurement.\n",
    "\n",
    "Bayes theorem is\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)}$$\n",
    "If you are not familiar with this notation, let's review. $P(A)$ means the probability of event $A$. If $A$ is the event of a fair coin landing heads, then $P(A) = 0.5$.\n",
    "\n",
    "$P(A \\mid B)$ is called a conditional probability. That is, it represents the probability of $A$ happening if $B$ happened. For example, it is more likely to rain today if it also rained yesterday because rain systems usually last more than one day. We'd write the probability of it raining today given that it rained yesterday as $P(\\mathtt{rain\\_today} \\mid \\mathtt{rain\\_yesterday})$.\n",
    "\n",
    "In the Bayes theorem equation above $B$ is the evidence, $P(A)$ is the prior, $P(B \\mid A)$ is the likelihood, and $P(A \\mid B)$ is the posterior. By substituting the mathematical terms with the corresponding words you can see that Bayes theorem matches out update equation. Let's rewrite the equation in terms of our problem. We will use $x_i$ for the position at i, and $Z$ for the measurement. Hence, we want to know $P(x_i \\mid Z)$, that is, the probability of the dog being at $x_i$ given the measurement $Z$.\n",
    "\n",
    "So, let's plug that into the equation and solve it.\n",
    "\n",
    "$$P(x_i \\mid Z) = \\frac{P(Z \\mid x_i) P(x_i)}{P(Z)}$$\n",
    "That looks ugly, but it is actually quite simple. Let's figure out what each term on the right means. First is $P(Z \\mid x_i)$. This is the the likelihood, or the probability for the measurement at every cell $x_i$. $P(x_i)$ is the prior - our belief before incorporating the measurements. We multiply those together. This is just the unnormalized multiplication in the update() function:\n",
    "\n",
    "```python\n",
    "def update(likelihood, prior):\n",
    "    posterior = prior * likelihood   # P(Z|x)*P(x)\n",
    "    return normalize(posterior)\n",
    "```    \n",
    "\n",
    "The last term to consider is the denominator $P(Z)$. This is the probability of getting the measurement $Z$ without taking the location into account. It is often called the evidence. We compute that by taking the sum of $x$, or sum(belief) in the code. That is how we compute the normalization! So, the update() function is doing nothing more than computing Bayes theorem.\n",
    "\n",
    "The literature often gives you these equations in the form of integrals. After all, an integral is just a sum over a continuous function. So, you might see Bayes' theorem written as\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{\\int P(B \\mid A_j) P(A_j) \\mathtt{d}A_j}\\cdot$$\n",
    "In practice the denominator can be fiendishly difficult to solve analytically (a recent opinion piece for the Royal Statistical Society called it a \"dog's breakfast\". Filtering textbooks are filled with integral laden equations which you cannot be expected to solve. We will learn more techniques to handle this in the Particle Filters chapter. Until then, recognize that in practice it is just a normalization term over which we can sum. What I'm trying to say is that when you are faced with a page of integrals, just think of them as sums, and relate them back to this chapter, and often the difficulties will fade. Ask yourself \"why are we summing these values\", and \"why am I dividing by this term\". Surprisingly often the answer is readily apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Probability Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the formal mathematics behind the update() function; what about the predict() function? predict() implements the total probability theorem. Let's recall what predict() computed. It computed the probability of being at any given position given the probability of all the possible movement events. Let's express that as an equation. The probability of being at any position $i$ at time $t$ can be written as $P(X_i^t)$. We computed that as the sum of the prior at time $t-1$ $P(X_j^{t-1})$ multiplied by the probability of moving from cell $x_j$ to $x_i$. That is\n",
    "\n",
    "$$P(X_i^t) = \\sum_j P(X_j^{t-1})  P(x_i | x_j)$$\n",
    "That equation is called the total probability theorem. Quoting from Wikipedia [6] \"It expresses the total probability of an outcome which can be realized via several distinct events\". I could have given you that equation and implemented predict(), but your chances of understanding why the equation works would be slim. As a reminder, here is the code that computes this equation\n",
    "\n",
    "```python\n",
    "for i in range(N):\n",
    "    for k in range (kN):\n",
    "        index = (i + (width-k) - offset) % N\n",
    "        result[i] += prob_dist[index] * kernel[k]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
